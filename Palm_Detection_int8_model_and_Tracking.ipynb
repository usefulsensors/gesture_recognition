{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Environment Setup"
      ],
      "metadata": {
        "id": "1q-EJsFlD1V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import csv\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "tLsoY7InEqT4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clone usefulsensors private repository to get quantized tflite models"
      ],
      "metadata": {
        "id": "6CKfumn3D5Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -fr gesture_recognition\n",
        "!git clone https://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@github.com/nyadla-sys/gesture_recognition.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPDtlI_VBNHM",
        "outputId": "1d67bd9b-cf2a-48ef-bf87-f5411ca00827"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gesture_recognition'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Quantized Tflite model"
      ],
      "metadata": {
        "id": "RjC8HslEJEMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"/content/gesture_recognition/hello.png\""
      ],
      "metadata": {
        "id": "2YTT_gX8GbCt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"/content/gesture_recognition/model_full_integer_quant_192x192.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "ans = []"
      ],
      "metadata": {
        "id": "QIGCc61GU2BI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_suppression_fast(boxes, probabilities=None, overlap_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Algorithm to filter bounding box proposals by removing the ones with a too low confidence score\n",
        "    and with too much overlap.\n",
        "    Source: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
        "    :param boxes: List of proposed bounding boxes\n",
        "    :param overlap_threshold: the maximum overlap that is allowed\n",
        "    :return: filtered boxes\n",
        "    \"\"\"\n",
        "    # if there are no boxes, return an empty list\n",
        "    if boxes.shape[1] == 0:\n",
        "        return []\n",
        "    # if the bounding boxes integers, convert them to floats --\n",
        "    # this is important since we'll be doing a bunch of divisions\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "    # initialize the list of picked indexes\n",
        "    pick = []\n",
        "    # grab the coordinates of the bounding boxes\n",
        "    x1 = boxes[:, 0] - (boxes[:, 2] / [2])  # center x - width/2\n",
        "    y1 = boxes[:, 1] - (boxes[:, 3] / [2])  # center y - height/2\n",
        "    x2 = boxes[:, 0] + (boxes[:, 2] / [2])  # center x + width/2\n",
        "    y2 = boxes[:, 1] + (boxes[:, 3] / [2])  # center y + height/2\n",
        "\n",
        "    # compute the area of the bounding boxes and grab the indexes to sort\n",
        "    # (in the case that no probabilities are provided, simply sort on the\n",
        "    # bottom-left y-coordinate)\n",
        "    area = boxes[:, 2] * boxes[:, 3]  # width * height\n",
        "    idxs = y2\n",
        "\n",
        "\n",
        "    # if probabilities are provided, sort on them instead\n",
        "    if probabilities is not None:\n",
        "        idxs = probabilities\n",
        "\n",
        "    # sort the indexes\n",
        "    idxs = np.argsort(idxs)\n",
        "    # keep looping while some indexes still remain in the indexes\n",
        "    # list\n",
        "    while len(idxs) > 0:\n",
        "        # grab the last index in the indexes list and add the\n",
        "        # index value to the list of picked indexes\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "        # find the largest (x, y) coordinates for the start of\n",
        "        # the bounding box and the smallest (x, y) coordinates\n",
        "        # for the end of the bounding box\n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        "        # compute the width and height of the bounding box\n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "        # compute the ratio of overlap\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "        # delete all indexes from the index list that have\n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "                                               np.where(overlap > overlap_threshold)[0])))\n",
        "    # return only the bounding boxes that were picked\n",
        "    return pick"
      ],
      "metadata": {
        "id": "CRF-DYMHxnoS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(file)\n",
        "image = cv2.resize(image, (192, 192))\n",
        "image = cv2.flip(image, 1)\n",
        "print(image.shape)\n",
        "# Convert the BGR image to RGB before processing.\n",
        "img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "img = np.expand_dims(img, axis = 0)/255.0\n",
        "img = np.asarray(img, dtype = np.int8)\n",
        "interpreter.set_tensor(input_details[0]['index'], img)\n",
        "interpreter.invoke()\n",
        "\n",
        "result = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "def _sigm(x):\n",
        "    return 1 / (1 + np.exp(-x) )\n",
        "\n",
        "# reading the SSD anchors\n",
        "with open(\"/content/gesture_recognition/anchors.csv\", \"r\") as csv_f:\n",
        "    anchors = np.r_[\n",
        "        [x for x in csv.reader(csv_f, quoting=csv.QUOTE_NONNUMERIC)]\n",
        "    ]\n",
        "\n",
        "#print(result)\n",
        "output_details = interpreter.get_output_details()\n",
        "print(\"%d outputs\" % len(output_details))\n",
        "for index, output_detail in enumerate(output_details):\n",
        "  print(\"  Output %d\" % index)\n",
        "  print(\"    name:\", output_detail['name'])\n",
        "  print(\"    shape:\", output_detail['shape'])\n",
        "  print(\"    type:\", output_detail['dtype'])\n",
        "\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "classification_out = interpreter.get_tensor(output_details[1]['index'])\n",
        "print(output_data.shape)\n",
        "print(classification_out.shape)\n",
        "\n",
        "out_reg_idx = output_details[0]['index']\n",
        "out_clf_idx = output_details[1]['index']\n",
        "out_clf = interpreter.get_tensor(out_clf_idx)[0,:,0]\n",
        "out_reg = interpreter.get_tensor(out_reg_idx)[0]\n",
        "#print(out_clf)\n",
        "# finding the best prediction\n",
        "probabilities = _sigm(out_clf)\n",
        "detecion_mask = probabilities > 0.5\n",
        "candidate_detect = out_reg[detecion_mask]\n",
        "candidate_anchors = anchors[detecion_mask]\n",
        "probabilities = probabilities[detecion_mask]\n",
        "#print(probabilities)\n",
        "if candidate_detect.shape[0] != 0:\n",
        "    print(\"hand found\")\n",
        "\n",
        "# Pick the first detected hand. Could be adapted for multi hand recognition\n",
        "# Pick the best bounding box with non maximum suppression\n",
        "# the boxes must be moved by the corresponding anchor first\n",
        "moved_candidate_detect = candidate_detect.copy()\n",
        "#moved_candidate_detect[:, :2] = candidate_detect[:, :2] + (candidate_anchors[:, :2] * 192)\n",
        "#box_ids = non_max_suppression_fast(moved_candidate_detect[:, :4], probabilities)\n",
        "#box_ids = box_ids[0]\n",
        "#print(probabilities)\n",
        "#print(detecion_mask)\n",
        "#print(candidate_detect)\n",
        "#candidate_anchors = anchors[detecion_mask]\n",
        "#probabilities = probabilities[detecion_mask]\n"
      ],
      "metadata": {
        "id": "7ot_5lwDUNgS",
        "outputId": "cc9c7ebf-8e97-4ee1-ce25-9da6bd7cb2c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(192, 192, 3)\n",
            "2 outputs\n",
            "  Output 0\n",
            "    name: Identity_1:0\n",
            "    shape: [   1 2016    1]\n",
            "    type: <class 'numpy.int8'>\n",
            "  Output 1\n",
            "    name: Identity:0\n",
            "    shape: [   1 2016   18]\n",
            "    type: <class 'numpy.int8'>\n",
            "(1, 2016, 1)\n",
            "(1, 2016, 18)\n",
            "hand found\n"
          ]
        }
      ]
    }
  ]
}