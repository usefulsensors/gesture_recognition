{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwjFeCsG5ho2",
        "outputId": "a5f7b505-b892-4aa7-93a7-307d3bd25f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gesture_recognition'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 86 (delta 9), reused 0 (delta 0), pack-reused 68\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/usefulsensors/gesture_recognition.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numpy opencv-python matplotlib tensorflow"
      ],
      "metadata": {
        "id": "xgRSQovH5t-V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import csv\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ZsG7FpT16FOk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.precision\", 3)\n",
        "file = \"/content/gesture_recognition/hello.png\"\n",
        "commands = [\"up\",\"down\", \"left\", \"right\", \"leftclick\", \"rightclick\", \"scroll up\", \"scroll down\"]\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/gesture_recognition/model_metadata.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "image = cv2.imread(file)\n",
        "image = cv2.resize(image, (224, 224))\n",
        "image = cv2.flip(image, 1)\n",
        "#print(image.shape)\n",
        "# Convert the BGR image to RGB before processing.\n",
        "img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "img = np.expand_dims(img, axis = 0)/255.0\n",
        "img = np.asarray(img, dtype = np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], img)\n",
        "interpreter.invoke()\n",
        "\n",
        "result = interpreter.get_tensor(output_details[0]['index'])\n",
        "#print(result)\n",
        "#print(np.argmax(result[0]))\n",
        "tflite_pred_dataframe = pd.DataFrame(result)\n",
        "tflite_pred_dataframe.columns = commands\n",
        "print(tflite_pred_dataframe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GAFXjYL6Jfy",
        "outputId": "0e1ca2da-818d-45c1-9522-52449a14324e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      up   down   left  right  leftclick  rightclick  scroll up  scroll down\n",
            "0  0.006  0.308  0.003  0.182      0.013       0.079      0.014        0.394\n"
          ]
        }
      ]
    }
  ]
}